stages:
  #  - lint
  - build
  - push
  - deploy
  - cleanup

# add the lint stage to verify the code structure and syntax. This will abort the pipeline if there are problems

    
# this lint stage is non-blocking and will not abort, but just report the INFO to the gitlab console


    
build:
  stage: build
  script:
    # this one is for running Dockerfile which is python wrapper method (and it ai parallelized now). It is 
    # a manual script running of the python files natively in parallel (not using multi-threading python class)
    # using a file range(array list) function
    #- docker build -t $CI_REGISTRY_IMAGE:$CI_PIPELINE_IID-$CI_COMMIT_SHORT_SHA -t $CI_REGISTRY_IMAGE:latest .
    
    # this one is for running Dockerfile_python_mod_NO_but_serial, the python modularization setup without parallelization
    #- docker build --file Dockerfile_python_mod_NO_but_serial -t $CI_REGISTRY_IMAGE:$CI_PIPELINE_IID-$CI_COMMIT_SHORT_SHA -t $CI_REGISTRY_IMAGE:latest .

    # this one is for running Dockerfile_python_mod_multi_threaded, modularization with multi-threading
    #- docker build --file Dockerfile_python_mod_multi_threaded -t $CI_REGISTRY_IMAGE:$CI_PIPELINE_IID-$CI_COMMIT_SHORT_SHA -t $CI_REGISTRY_IMAGE:latest .
  
    # this one is for running Dockerfile_python_mod_multi_processing, modularization with mutli-processing
    - docker build --file Dockerfile_python_mod_multi_processing -t $CI_REGISTRY_IMAGE:$CI_PIPELINE_IID-$CI_COMMIT_SHORT_SHA -t $CI_REGISTRY_IMAGE:latest .


push:
  stage: push
  script:
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
    - docker push $CI_REGISTRY_IMAGE:$CI_PIPELINE_IID-$CI_COMMIT_SHORT_SHA
    - docker push $CI_REGISTRY_IMAGE:latest


      # default instance_type is t2.micro. Test out a t3.small for performance installation issues.      
deploy:
  stage: deploy
  before_script:
    - echo 'AWS_ACCESS_KEY_ID='${AWS_ACCESS_KEY_ID} >> .env
    - echo 'AWS_SECRET_ACCESS_KEY='${AWS_SECRET_ACCESS_KEY} >> .env
    - echo 'region_name=us-east-1' >> .env
    - echo 'image_id=ami-0f9de6e2d2f067fca' >> .env
    - echo 'instance_type=t3.small' >> .env
    - echo 'key_name=generic_keypair_for_python_testing' >> .env
    - echo 'min_count=50' >> .env
    - echo 'max_count=50' >> .env
    - echo 'AWS_PEM_KEY='${AWS_PEM_KEY} >> .env
    - echo 'DB_USERNAME='${DB_USERNAME} >> .env
    - echo 'DB_PASSWORD='${DB_PASSWORD} >> .env

  script:
    - docker run --rm --env-file .env $CI_REGISTRY_IMAGE:latest
  allow_failure: true
  # this will keep the pipeline going to cleanup stage even if the above python script  fails
  only:
    - main

cleanup:
  stage: cleanup
  script:
    - docker rmi $CI_REGISTRY_IMAGE:$CI_PIPELINE_IID-$CI_COMMIT_SHORT_SHA $CI_REGISTRY_IMAGE:latest -f
  when: always
