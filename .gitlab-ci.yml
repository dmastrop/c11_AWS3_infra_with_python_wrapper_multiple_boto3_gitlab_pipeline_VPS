stages:
  #  - lint
  - build
  - push
  - deploy
  - cleanup

# add the lint stage to verify the code structure and syntax. This will abort the pipeline if there are problems

    
# this lint stage is non-blocking and will not abort, but just report the INFO to the gitlab console


    
build:
  stage: build
  script:
    # this one is for running Dockerfile which is python wrapper method (and it ai parallelized now). It is 
    # a manual script running of the python files natively in parallel (not using multi-threading python class)
    # using a file range(array list) function
    #- docker build -t $CI_REGISTRY_IMAGE:$CI_PIPELINE_IID-$CI_COMMIT_SHORT_SHA -t $CI_REGISTRY_IMAGE:latest .
    
    # this one is for running Dockerfile_python_mod_NO_but_serial, the python modularization setup without parallelization
    #- docker build --file Dockerfile_python_mod_NO_but_serial -t $CI_REGISTRY_IMAGE:$CI_PIPELINE_IID-$CI_COMMIT_SHORT_SHA -t $CI_REGISTRY_IMAGE:latest .

    # this one is for running Dockerfile_python_mod_multi_threaded, modularization with multi-threading
    #- docker build --file Dockerfile_python_mod_multi_threaded -t $CI_REGISTRY_IMAGE:$CI_PIPELINE_IID-$CI_COMMIT_SHORT_SHA -t $CI_REGISTRY_IMAGE:latest .
  
    # this one is for running Dockerfile_python_mod_multi_processing, modularization with mutli-processing
    - docker build --file Dockerfile_python_mod_multi_processing -t $CI_REGISTRY_IMAGE:$CI_PIPELINE_IID-$CI_COMMIT_SHORT_SHA -t $CI_REGISTRY_IMAGE:latest .


push:
  stage: push
  script:
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
    - docker push $CI_REGISTRY_IMAGE:$CI_PIPELINE_IID-$CI_COMMIT_SHORT_SHA
    - docker push $CI_REGISTRY_IMAGE:latest


      # default instance_type is t2.micro. Test out a t3.small for performance installation issues.      
deploy:
  stage: deploy
  before_script:
    - echo 'AWS_ACCESS_KEY_ID='${AWS_ACCESS_KEY_ID} >> .env
    - echo 'AWS_SECRET_ACCESS_KEY='${AWS_SECRET_ACCESS_KEY} >> .env
    - echo 'region_name=us-east-1' >> .env
    - echo 'image_id=ami-0f9de6e2d2f067fca' >> .env
    - echo 'instance_type=t2.micro' >> .env
    - echo 'key_name=generic_keypair_for_python_testing' >> .env
    - echo 'min_count=250' >> .env
    - echo 'max_count=250' >> .env
    - echo 'AWS_PEM_KEY='${AWS_PEM_KEY} >> .env
    - echo 'DB_USERNAME='${DB_USERNAME} >> .env
    - echo 'DB_PASSWORD='${DB_PASSWORD} >> .env


      #      # use this for most of the Dockerfiles. The multiprocessing Dockerfile is collecting logs from the container for
      #      # benchmarking so use the one below for the multiprocessing Dockerfile    
      #  script:
      #    - docker run --rm --env-file .env $CI_REGISTRY_IMAGE:latest
      #  allow_failure: true
      #  # this will keep the pipeline going to cleanup stage even if the above python script  fails
      #  only:
      #    - main
      #




      # use this one for the multiprocessing Dockerfile. This mounts to the WORKDIR on the container so that
      # the gitlab pipeline can get the artifact from the $CI_PROJECT_DIR of the gitlab repo
      # The artifact path is specified below relative to the gitlab project directory
      # Refer to teh python module2 to see the per process logging setup to benchmark the multi-threading operations
      # in each process
      #
  script:
    - mkdir -p logs
    - docker run --rm --env-file .env -v $CI_PROJECT_DIR/logs:/aws_EC2/logs $CI_REGISTRY_IMAGE:latest
    - echo "Contents of logs directory after container run:"
    - ls -l logs/
    - |
      for f in logs/benchmark_*.log; do
        echo "===== $f =====" >> logs/benchmark_combined.log
        cat "$f" >> logs/benchmark_combined.log
        echo "" >> logs/benchmark_combined.log
      done


  artifacts:
    paths:
      - logs/
      - logs/benchmark_combined.log
    expire_in: 1 week

  allow_failure: true
  only:
    - main


    #script:
    #  - mkdir -p logs
    #  - docker run --rm --env-file .env -v $CI_PROJECT_DIR/logs:/aws_EC2/logs $CI_REGISTRY_IMAGE:latest
    #  - echo "Contents of logs directory after container run:"
    #  - ls -l logs/
    #  - cat logs/benchmark_*.log > logs/benchmark_combined.log
    #
    #artifacts:
    #  paths:
    #    - logs/
    #    - logs/benchmark_combined.log
    #  expire_in: 1 week
    #
    #allow_failure: true
    #only:
    #  - main
    #



cleanup:
  stage: cleanup
  script:
    - docker rmi $CI_REGISTRY_IMAGE:$CI_PIPELINE_IID-$CI_COMMIT_SHORT_SHA $CI_REGISTRY_IMAGE:latest -f
  when: always
